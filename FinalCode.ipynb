{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9e0013-1da5-488e-b2ae-c503e9c2575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 16:49:11.930825: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 16:49:16.230706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n",
      "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorrt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8200688-2868-4e8c-a7be-c2ec06761b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "\n",
    "class ConsumptionSavingsEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ConsumptionSavingsEnv, self).__init__()\n",
    "\n",
    "        # Environment parameters\n",
    "        self.T = 65 \n",
    "        self.r = 0.05\n",
    "        self.risk_aversion = 2\n",
    "        self.alpha = 0.5\n",
    "        \n",
    "        # Define observation space (state space)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(self.T+1),        # Time step t, integer from 0 to 65\n",
    "            spaces.Box(low=-1, high=10, shape=(1,))  # Wealth level a_t, a continuous value between -1 and 10\n",
    "        ))\n",
    "        \n",
    "        # Define action space\n",
    "        self.action_space = spaces.Discrete(220)  # Discrete actions from 0 to 219\n",
    "        \n",
    "        # Initialize the state at time t=0 with wealth a_t=0\n",
    "        self.state = (0,0)\n",
    "\n",
    "    def step(self, a_t1_index):\n",
    "        # Unpack the current state (time t, wealth a_t)\n",
    "        t, a_t = self.state\n",
    "        \n",
    "        # Convert the action index (0 to 219) to an actual action value between -1 and 1\n",
    "        a_t1 = (a_t1_index - 20) / 20\n",
    "        \n",
    "        # Initialize the reward at the beginning of each step\n",
    "        reward = 0\n",
    "        \n",
    "        # Calculate the time-dependent y_t value using the alpha parameter\n",
    "        y_t = 1 / (1 + np.exp(-self.alpha * t)) if t <= 65 else 0\n",
    "        \n",
    "        # Calculate the maximum feasible wealth at time t+1, given the current wealth a_t\n",
    "        max_feasible_a_t1 = (1 + self.r) * a_t + y_t \n",
    "        \n",
    "        # Check if the action leads to an infeasible wealth value, and penalize if it does\n",
    "        if a_t1 > max_feasible_a_t1:\n",
    "            a_t1 = max_feasible_a_t1\n",
    "            reward = -1e3\n",
    "        \n",
    "        # Calculate the consumption at time t\n",
    "        c_t = (1+self.r)*a_t + y_t - a_t1\n",
    "        \n",
    "        # Calculate the reward based on the consumption and risk aversion parameters\n",
    "        if c_t <= 0:\n",
    "            reward = -1e3\n",
    "        elif self.risk_aversion == 1:\n",
    "            reward = np.log(c_t)\n",
    "        else:\n",
    "            reward = (c_t**(1 - self.risk_aversion)-1) / (1 - self.risk_aversion)\n",
    "\n",
    "        # Check if the episode is done (time step reaches the final T=65)\n",
    "        done = t == self.T\n",
    "        # If the episode is done, add the final wealth to the reward\n",
    "        if done:\n",
    "            reward += a_t1\n",
    "\n",
    "        # Update the state for the next time step (t+1) and return the updated state, reward, done flag, and an empty dictionary\n",
    "        self.state = (t+1, a_t1)\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the state to the initial state (t=0, a_t=0) at the beginning of each episode\n",
    "        self.state = (0, 0)\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Print the current time step and wealth for visualization\n",
    "        print(f\"Time: {self.state[0]}, Wealth: {self.state[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0be8c30-3586-429e-b97f-dd60b3d00846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import clone_model\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, memory_size=1000, tau=0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Experience replay memory to store past experiences\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "        # Recent scores deque to track the last 100 episode scores\n",
    "        self.recent_scores = deque(maxlen=100)\n",
    "\n",
    "        # Discount factor (gamma) for the Bellman equation\n",
    "        self.gamma = np.exp(-0.04)\n",
    "\n",
    "        # Exploration-exploitation trade-off (epsilon-greedy) parameters\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.95\n",
    "\n",
    "        # Learning rate for the optimizer\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "        # Soft update rate for the target model\n",
    "        self.tau = tau\n",
    "\n",
    "        # Create the main and target neural network models\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = clone_model(self.model)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Build a simple neural network model using Keras\n",
    "        model = Sequential()\n",
    "        model.add(Dense(5, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(5, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done, score):\n",
    "        # Store the experience tuple (state, action, reward, next_state, done) in the replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "        # Keep track of the recent episode scores\n",
    "        self.recent_scores.append(score)\n",
    "\n",
    "    def adapt_epsilon(self):\n",
    "        # Adapt the epsilon value based on the standard deviation of recent scores\n",
    "        if len(self.recent_scores) == 100:\n",
    "            std_dev = np.std(self.recent_scores)\n",
    "            if std_dev > 0.05:\n",
    "                self.epsilon = min(self.epsilon * 1.01, 1.0)\n",
    "            else:\n",
    "                self.epsilon = max(self.epsilon * 0.99, self.epsilon_min)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy policy: Explore (random action) or Exploit (best predicted action)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        # Choose randomly among the actions with the maximum Q-value\n",
    "        max_value = np.max(act_values[0])\n",
    "        max_indices = [i for i, v in enumerate(act_values[0]) if v == max_value]\n",
    "        return np.random.choice(max_indices)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # Experience replay to learn from past experiences\n",
    "\n",
    "        # Sample a mini-batch from the replay memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.target_model.predict(state, verbose=0)\n",
    "\n",
    "            if done:\n",
    "                # If the episode is done, the target value is just the immediate reward\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                # If the episode is not done, update the target value using the Bellman equation\n",
    "                Q_future = max(self.target_model.predict(next_state, verbose=0)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "\n",
    "            # Perform one step of gradient descent to minimize the loss (MSE)\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "        # Perform a soft update of the target model's weights using a fraction (tau) of the main model's weights\n",
    "        self.soft_update(self.model, self.target_model, self.tau)\n",
    "\n",
    "    def soft_update(self, model, target_model, tau):\n",
    "        # Update the target model's weights softly based on tau\n",
    "\n",
    "        model_weights = model.get_weights()\n",
    "        target_weights = target_model.get_weights()\n",
    "\n",
    "        for i in range(len(model_weights)):\n",
    "            target_weights[i] = tau * model_weights[i] + (1 - tau) * target_weights[i]\n",
    "\n",
    "        target_model.set_weights(target_weights)\n",
    "\n",
    "    def load(self, name):\n",
    "        # Load the model's weights from a file\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        # Save the model's weights to a file\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a409c5-7544-4858-83c6-45f22ccf5bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 16:49:33.993364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14587 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0001:00:00.0, compute capability: 7.0\n",
      "2023-08-02 16:49:33.999884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14587 MB memory:  -> device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0002:00:00.0, compute capability: 7.0\n",
      "2023-08-02 16:49:34.001198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14587 MB memory:  -> device: 2, name: Tesla V100-PCIE-16GB, pci bus id: 0003:00:00.0, compute capability: 7.0\n",
      "2023-08-02 16:49:34.002498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 14587 MB memory:  -> device: 3, name: Tesla V100-PCIE-16GB, pci bus id: 0004:00:00.0, compute capability: 7.0\n",
      "2023-08-02 16:49:44.080743: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x412bfc90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-02 16:49:44.080808: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0\n",
      "2023-08-02 16:49:44.080818: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): Tesla V100-PCIE-16GB, Compute Capability 7.0\n",
      "2023-08-02 16:49:44.080824: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): Tesla V100-PCIE-16GB, Compute Capability 7.0\n",
      "2023-08-02 16:49:44.080830: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): Tesla V100-PCIE-16GB, Compute Capability 7.0\n",
      "2023-08-02 16:49:44.412525: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-02 16:49:45.010374: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-08-02 16:49:45.010465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 470.182.3\n",
      "2023-08-02 16:49:45.137046: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-08-02 16:49:45.137428: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-08-02 16:49:45.137475: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "\t [[{{node StatefulPartitionedCall_4}}]]\n",
      "2023-08-02 16:49:45.153410: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-08-02 16:49:45.153496: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 470.182.3\n",
      "2023-08-02 16:49:45.176236: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-08-02 16:49:45.176442: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-08-02 16:49:45.193713: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-08-02 16:49:45.193777: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 470.182.3\n",
      "2023-08-02 16:49:45.216340: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-08-02 16:49:45.216528: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-08-02 16:49:45.230109: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-08-02 16:49:45.230167: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 470.182.3\n",
      "2023-08-02 16:49:45.252822: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-08-02 16:49:45.252993: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-08-02 16:49:45.266802: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-08-02 16:49:45.266864: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 470.182.3\n",
      "2023-08-02 16:49:45.289661: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-08-02 16:49:45.289853: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-08-02 16:49:45.303320: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:429] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED\n",
      "2023-08-02 16:49:45.303383: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:438] Possibly insufficient driver version: 470.182.3\n",
      "2023-08-02 16:49:45.326164: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-08-02 16:49:45.326424: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_4' defined at (most recent call last):\n    File \"/anaconda/envs/tfgpu/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/anaconda/envs/tfgpu/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_6167/1254737597.py\", line 53, in <module>\n      agent.replay(BATCH_SIZE)\n    File \"/tmp/ipykernel_6167/4160052766.py\", line 92, in replay\n      self.model.fit(state, target, epochs=1, verbose=0)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_4'\nRET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n\t [[{{node StatefulPartitionedCall_4}}]] [Op:__inference_train_function_1141]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m     agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done, score)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m BATCH_SIZE:\n\u001b[0;32m---> 53\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Move to the next state\u001b[39;00m\n\u001b[1;32m     56\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[3], line 92\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m         target[\u001b[38;5;241m0\u001b[39m][action] \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m Q_future \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Perform one step of gradient descent to minimize the loss (MSE)\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Perform a soft update of the target model's weights using a fraction (tau) of the main model's weights\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoft_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n",
      "File \u001b[0;32m/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/anaconda/envs/tfgpu/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_4' defined at (most recent call last):\n    File \"/anaconda/envs/tfgpu/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/anaconda/envs/tfgpu/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_6167/1254737597.py\", line 53, in <module>\n      agent.replay(BATCH_SIZE)\n    File \"/tmp/ipykernel_6167/4160052766.py\", line 92, in replay\n      self.model.fit(state, target, epochs=1, verbose=0)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/anaconda/envs/tfgpu/lib/python3.10/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_4'\nRET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n\t [[{{node StatefulPartitionedCall_4}}]] [Op:__inference_train_function_1141]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define some parameters\n",
    "EPISODES = 4000\n",
    "WARMUP_EPISODES = 1000\n",
    "BATCH_SIZE = 23\n",
    "\n",
    "# Instantiate environment and agent\n",
    "env = ConsumptionSavingsEnv()\n",
    "state_size = 2\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Lists to store the best rewards and corresponding consumption and savings paths\n",
    "best_lifetime_rewards = []\n",
    "best_consumption_paths = []\n",
    "best_savings_paths = []\n",
    "episode_rewards = []\n",
    "rewards_time = []\n",
    "\n",
    "# Loop over episodes\n",
    "for e in range(1, EPISODES+1):\n",
    "    # Reset the environment and get the initial state\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    done = False\n",
    "    score = 0\n",
    "    consumption_path = []\n",
    "    savings_path = []\n",
    "    \n",
    "    # Execute the episode until it's done\n",
    "    while not done:\n",
    "        # Select an action using the agent's epsilon-greedy policy\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Take the selected action and observe the next state, reward, and done flag\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        # Calculate consumption and savings for visualization\n",
    "        consumption = (1+env.r)*state[0][1] + 1/(1 + np.exp(-env.alpha * state[0][0])) - next_state[0][1]\n",
    "        savings = next_state[0][1]\n",
    "        consumption_path.append(consumption)\n",
    "        savings_path.append(savings)\n",
    "        \n",
    "        # Update the total score for the episode\n",
    "        score += reward\n",
    "        \n",
    "        # If we are beyond the warm-up episodes, store the experience and perform learning\n",
    "        if e > WARMUP_EPISODES:\n",
    "            agent.remember(state, action, reward, next_state, done, score)\n",
    "            if len(agent.memory) > BATCH_SIZE:\n",
    "                agent.replay(BATCH_SIZE)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "    # Record the total score for the episode\n",
    "    episode_rewards.append(score)\n",
    "    \n",
    "    # Adapt the epsilon value based on recent scores after the warm-up period\n",
    "    agent.adapt_epsilon()\n",
    "    \n",
    "    # Save reward storing over time\n",
    "    savings_path.append(max(episode_rewards))\n",
    "    \n",
    "    # Print episode information every 100 episodes after warm-up\n",
    "    if (e > WARMUP_EPISODES):\n",
    "        print(\"Episode: {}, Total Reward: {}\".format(e, score), end='', flush=True)\n",
    "        \n",
    "\n",
    "    # Save the best rewards and corresponding consumption and savings paths every 300 episodes after warm-up\n",
    "    if (e > WARMUP_EPISODES) & (e % 300 == 0):\n",
    "        print(f\"episode: {e}/{EPISODES}, score: {score}\")\n",
    "        best_lifetime_rewards.append(max(episode_rewards))\n",
    "        best_consumption_paths.append(consumption_path)\n",
    "        best_savings_paths.append(savings_path)\n",
    "        # Plot the consumption and savings paths for visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(best_consumption_paths[-1], label=\"Best consumption path\")\n",
    "        plt.plot(best_savings_paths[-1], label=\"Best savings path\")\n",
    "        plt.title(f\"Training episode {e} - Reward {best_lifetime_rewards[-1]}\")\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amount')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"training_episode_{e}.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2908e-4bd0-42c3-b3cc-2f2410b61ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#print(f\"episode: {e}/{EPISODES}, score: {score}\")\n",
    "#best_lifetime_rewards.append(max(episode_rewards))\n",
    "#best_consumption_paths.append(consumption_path)\n",
    "#best_savings_paths.append(savings_path)\n",
    "## plot the paths\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#plt.plot(best_consumption_paths[-1], label=\"Best consumption path\")\n",
    "#plt.plot(best_savings_paths[-1], label=\"Best savings path\")\n",
    "#plt.title(f\"Training episode {e} - Reward {best_lifetime_rewards[-1]}\")\n",
    "#plt.xlabel('Time')\n",
    "#plt.ylabel('Amount')\n",
    "#plt.legend()\n",
    "#plt.savefig(f\"training_episode_{e}.png\")\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
